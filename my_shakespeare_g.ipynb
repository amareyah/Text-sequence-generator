{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sample_to_hot_vector_and_target_indexes(sample,dictionary,dic_length):\n",
    "    #here x[0] is all zeros row, it is used as x0=0 as initial input to sequence generation\n",
    "    x = torch.zeros(len(sample)+1,dic_length)\n",
    "    y = torch.empty((len(sample)),dtype=torch.long)\n",
    "    \n",
    "    for j,ch in enumerate(sample):\n",
    "        k = chr_to_idx[ch]\n",
    "        x[j+1,k]=1.0\n",
    "        y[j] = k\n",
    "    return x[:-1], y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_samples_to_hot_tensors(samples_list,chr_to_idx_dict):\n",
    "    \n",
    "    dic_length = len(chr_to_idx_dict)\n",
    "    x_train_list = []\n",
    "    \n",
    "    for i, txt in enumerate(samples_list):\n",
    "        x,y = encode_sample_to_hot_vector_and_target_indexes(txt,chr_to_idx_dict,dic_length)\n",
    "        x_train_list.append((x,y))\n",
    "\n",
    "    #x_train_list.sort(key=lambda val: val[0].shape[0],reverse=True)\n",
    "    \n",
    "    return x_train_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(samples_list,batch_size):\n",
    "    total_samples_num = len(samples_list)    \n",
    "    #total_samples_num = 150\n",
    "\n",
    "    batch_number = int(total_samples_num/batch_size)+int(total_samples_num%batch_size>0)\n",
    "    \n",
    "    rand_indexes = torch.randperm(total_samples_num)\n",
    "    \n",
    "    for i in range(batch_number):\n",
    "        rand_indexes_for_batch = rand_indexes[batch_size*i:batch_size*(i+1)]\n",
    "        batch_samples = [samples_list[j] for j in rand_indexes_for_batch]\n",
    "        batch_samples.sort(key=lambda s: s[0].shape[0],reverse=True)\n",
    "        \n",
    "        yield batch_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_preprocess(batch):\n",
    "    x_list = [sample[0] for sample in batch]\n",
    "    y_list = [sample[1] for sample in batch]\n",
    "    \n",
    "    x_train_batch = torch.nn.utils.rnn.pack_sequence(x_list)\n",
    "    y_target_batch = torch.nn.utils.rnn.pack_sequence(y_list)\n",
    "    \n",
    "    return x_train_batch, y_target_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_module(nn.Module):\n",
    "    def __init__(self,hidd_size,num_layers,is_bidirect, dictionary):\n",
    "        super().__init__()\n",
    "        # number of lstm stacked layers\n",
    "        self.num_layers = num_layers \n",
    "        self.input_size = len(dictionary)\n",
    "        self.hidden_size = hidd_size\n",
    "        \n",
    "        # number of directions in lstm\n",
    "        self.num_directions = 2 if is_bidirect else 1 \n",
    "        \n",
    "        self.idx_to_chr = dictionary\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=self.input_size, hidden_size=self.hidden_size,num_layers=num_layers,bias=True,batch_first=False,bidirectional=is_bidirect)\n",
    "        self.linear = nn.Linear(self.hidden_size*self.num_directions,self.input_size)\n",
    "    \n",
    "    def _forward(self,x): \n",
    "        #x.shape --> (sequence_length,batch,features_size)\n",
    "        #lstm_out.shape --> (sequence_length,batch,hidden_size)\n",
    "        #h_n, c_n shape --> (num_layers * num_directions, batch, hidden_size)\n",
    "        lstm_out, (h_n, c_n) = self.lstm(x) \n",
    "        \n",
    "        #y_hat shape --> (sequence_length,batch,output_size=input_size)\n",
    "        y_hat = self.linear(lstm_out.data)\n",
    "        \n",
    "        return y_hat,(h_n, c_n)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self._forward(x)\n",
    "        \n",
    "    def generate_samples(self,samples_number_to_generate, max_sample_length):\n",
    "        #x.shape --> (sequence_length,batch,features_size)\n",
    "        #lstm_out.shape --> (sequence_length,batch,hidden_size)\n",
    "        #h_n, c_n shape --> (num_layers * num_directions, batch, hidden_size)\n",
    "        x = torch.zeros(1,1,self.input_size)\n",
    "        h_t = c_t = torch.zeros((self.num_layers*self.num_directions,1,self.hidden_size))\n",
    "        \n",
    "        #generated samples are appended to this list\n",
    "        generated_samples_list=[] \n",
    "        \n",
    "        #this list is used for constracting one sample\n",
    "        generated_sample = []     \n",
    "        \n",
    "        #this loop is used to generate desired number of samples\n",
    "        for i in range(samples_number_to_generate): \n",
    "            #here we zero out initial input and initial hidden and state variables\n",
    "            x.zero_()\n",
    "            h_t.zero_()\n",
    "            c_t.zero_()\n",
    "    \n",
    "            for t in range(max_sample_length):\n",
    "                #forward propagating previously generated character given as vector x. \n",
    "                #(h_t,c_t) are hidden and memory state from previous iteration\n",
    "                \n",
    "                #lstm_out shape --> (sequence_length,batch,hidden_size)\n",
    "                #h_n, c_n shape --> (num_layers * num_directions, batch, hidden_size)\n",
    "                lstm_out, (h_t, c_t) = self.lstm(x,(h_t,c_t))\n",
    "                \n",
    "                #y_hat is unnormilized output of linear layer which is applied to output of lstm layer\n",
    "                #y_hat shape --> (sequence_length,batch,output_size=input_size)\n",
    "                y_hat = self.linear(lstm_out)\n",
    "                \n",
    "                #these are probabilities of possible characters (this is probability distribution generated by network)\n",
    "                #we need to squeeze y_hat to obtain one dimensional tensor\n",
    "                #p_model length is dictionary length\n",
    "                p_model = torch.nn.functional.softmax(y_hat,dim=2).squeeze()\n",
    "        \n",
    "                #here we sample index of charachter from probability distribution array\n",
    "                generated_char_ind = torch.multinomial(p_model,1).item()\n",
    "                \n",
    "                #find corresponding character in dictionary for sampled index \n",
    "                generated_char = self.idx_to_chr[generated_char_ind]\n",
    "                \n",
    "                #append this generated charachter to sample being generated. (we construct sample one by one character)\n",
    "                generated_sample.append(generated_char)\n",
    "        \n",
    "                #here we generate hot vector for already generated character and will use it as next input to network\n",
    "                x.zero_()\n",
    "                x[0,0,generated_char_ind]=1.\n",
    "                \n",
    "                #if network generates end of line character, we stop generation for this sample and begin for next sample.\n",
    "                if generated_char=='\\n' or generated_char=='.':\n",
    "                    break\n",
    "            #generate sample string and save that sample to samples list\n",
    "            generated_samples_list.append(''.join(generated_sample))\n",
    "        \n",
    "            #prepare for next sample construction\n",
    "            generated_sample.clear()\n",
    "        \n",
    "        return generated_samples_list\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_training_samples_num =  None\n",
    "batch_size = 8\n",
    "\n",
    "with open('shakespeare.txt') as f:\n",
    "    whole_text = f.read()\n",
    "    alpha = set(whole_text.lower())\n",
    "    del whole_text\n",
    "    idx_to_chr = {i:c for i,c in enumerate(alpha)}\n",
    "    chr_to_idx = {c:i for i,c in enumerate(alpha)}\n",
    "    f.seek(0)\n",
    "    samples = f.readlines()\n",
    "    samples = [l.lower() for i,l in enumerate(samples) \n",
    "               if total_training_samples_num is None or i < total_training_samples_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_size = len(alpha)\n",
    "hidden_size = 64\n",
    "output_size = dictionary_size\n",
    "num_layers = 2\n",
    "directions_num = 1\n",
    "is_bidirectional = directions_num == 2\n",
    "\n",
    "samples_number_to_generate = 20\n",
    "max_sample_length = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_tensor_list = encode_samples_to_hot_tensors(samples,chr_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module = LSTM_module(hidden_size,num_layers,is_bidirectional,idx_to_chr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can see here sample generation by untrained model\n",
    "module.generate_samples(samples_number_to_generate,max_sample_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criteria = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.Adam(module.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(20):\n",
    "    batch_gen = batch_generator(samples_tensor_list,batch_size)\n",
    "    \n",
    "    for batch in batch_gen:\n",
    "        x, y = batch_preprocess(batch)\n",
    "\n",
    "        y_hat, (h_n, c_n) = module(x)\n",
    "\n",
    "        loss = criteria(y_hat, y.data)\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "    print('epoch {} | loss {}'.format(epoch,loss.item()))\n",
    "print('finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#here is sample generation by trained model\n",
    "module.generate_samples(samples_number_to_generate,max_sample_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
