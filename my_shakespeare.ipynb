{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as d\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sample_to_hot_vector_and_target_indexes(sample,dictionary,dic_length):\n",
    "    #here x[0] is all zeros row, it is used as x0=0 as initial input to sequence generation\n",
    "    x = torch.zeros(len(sample)+1,dic_length)\n",
    "    y = torch.empty((len(sample)),dtype=torch.long)\n",
    "    \n",
    "    for j,ch in enumerate(sample):\n",
    "        k = chr_to_idx[ch]\n",
    "        x[j+1,k]=1.0\n",
    "        y[j] = k\n",
    "    return x[:-1], y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_samples_to_hot_tensors(samples_list,chr_to_idx_dict):\n",
    "    \n",
    "    dic_length = len(chr_to_idx_dict)\n",
    "    x_train_list = []\n",
    "    \n",
    "    for i, txt in enumerate(samples_list):\n",
    "        x,y = encode_sample_to_hot_vector_and_target_indexes(txt,chr_to_idx_dict,dic_length)\n",
    "        \n",
    "        #here each training sample is appended as tuple of sample x and target y\n",
    "        x_train_list.append((x,y))\n",
    "    \n",
    "    return x_train_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(samples_list,batch_size):\n",
    "    total_samples_num = len(samples_list)    \n",
    "\n",
    "    batch_number = int(total_samples_num/batch_size)+int(total_samples_num%batch_size>0)\n",
    "    \n",
    "    rand_indexes = torch.randperm(total_samples_num)\n",
    "    \n",
    "    for i in range(batch_number):\n",
    "        rand_indexes_for_batch = rand_indexes[batch_size*i:batch_size*(i+1)]\n",
    "        batch_samples = [samples_list[j] for j in rand_indexes_for_batch]\n",
    "        batch_samples.sort(key=lambda s: s[0].shape[0],reverse=True)\n",
    "        \n",
    "        yield batch_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_preprocess(batch):\n",
    "    x_list = [sample[0] for sample in batch]\n",
    "    y_list = [sample[1] for sample in batch]\n",
    "    \n",
    "    x_train_batch = torch.nn.utils.rnn.pack_sequence(x_list)\n",
    "    y_target_batch = torch.nn.utils.rnn.pack_sequence(y_list)\n",
    "    \n",
    "    return x_train_batch, y_target_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_module(nn.Module):\n",
    "    def __init__(self,hidd_size,num_layers,is_bidirect, dictionary):\n",
    "        super().__init__()\n",
    "        # number of lstm stacked layers\n",
    "        self.num_layers = num_layers \n",
    "        self.input_size = len(dictionary)\n",
    "        self.hidden_size = hidd_size\n",
    "        \n",
    "        # number of directions in lstm\n",
    "        self.num_directions = 2 if is_bidirect else 1 \n",
    "        \n",
    "        self.idx_to_chr = dictionary\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=self.input_size, hidden_size=self.hidden_size,num_layers=num_layers,bias=True,batch_first=False,bidirectional=is_bidirect)\n",
    "        self.linear = nn.Linear(self.hidden_size*self.num_directions,self.input_size)\n",
    "    \n",
    "    def forward(self,x): \n",
    "        #x.shape --> (sequence_length,batch,features_size)\n",
    "        #lstm_out.shape --> (sequence_length,batch,hidden_size)\n",
    "        #h_n, c_n shape --> (num_layers * num_directions, batch, hidden_size)\n",
    "        lstm_out, (h_n, c_n) = self.lstm(x) \n",
    "        \n",
    "        #y_hat shape --> (sequence_length,batch,output_size=input_size)\n",
    "        y_hat = self.linear(lstm_out.data)\n",
    "        \n",
    "        return y_hat,(h_n, c_n)\n",
    "    \n",
    "    #def forward(self,x):\n",
    "        #return self._forward(x)\n",
    "        \n",
    "    def generate_samples(self,samples_number_to_generate, max_sample_length):\n",
    "        #x.shape --> (sequence_length,batch,features_size)\n",
    "        #lstm_out.shape --> (sequence_length,batch,hidden_size)\n",
    "        #h_n, c_n shape --> (num_layers * num_directions, batch, hidden_size)\n",
    "        is_cuda = next(self.parameters()).is_cuda\n",
    "        device = torch.device('cuda' if is_cuda else 'cpu')\n",
    "        x = torch.zeros(1,1,self.input_size).to(device)\n",
    "        h_t = c_t = torch.zeros((self.num_layers*self.num_directions,1,self.hidden_size)).to(device)\n",
    "        \n",
    "        #generated samples are appended to this list\n",
    "        generated_samples_list=[] \n",
    "        \n",
    "        #this list is used for constracting one sample\n",
    "        generated_sample = []     \n",
    "        \n",
    "        #this loop is used to generate desired number of samples\n",
    "        for i in range(samples_number_to_generate): \n",
    "            #here we zero out initial input and initial hidden and state variables\n",
    "            x.zero_()\n",
    "            h_t.zero_()\n",
    "            c_t.zero_()\n",
    "    \n",
    "            for t in range(max_sample_length):\n",
    "                #forward propagating previously generated character given as vector x. \n",
    "                #(h_t,c_t) are hidden and memory state from previous iteration\n",
    "                \n",
    "                #lstm_out shape --> (sequence_length,batch,hidden_size)\n",
    "                #h_n, c_n shape --> (num_layers * num_directions, batch, hidden_size)\n",
    "                lstm_out, (h_t, c_t) = self.lstm(x,(h_t,c_t))\n",
    "                \n",
    "                #y_hat is unnormilized output of linear layer which is applied to output of lstm layer\n",
    "                #y_hat shape --> (sequence_length,batch,output_size=input_size)\n",
    "                y_hat = self.linear(lstm_out)\n",
    "                \n",
    "                #these are probabilities of possible characters (this is probability distribution generated by network)\n",
    "                #we need to squeeze y_hat to obtain one dimensional tensor\n",
    "                #p_model length is dictionary length\n",
    "                p_model = torch.nn.functional.softmax(y_hat,dim=2).squeeze()\n",
    "        \n",
    "                #here we sample index of charachter from probability distribution array\n",
    "                generated_char_ind = torch.multinomial(p_model,1).item()\n",
    "                \n",
    "                #find corresponding character in dictionary for sampled index \n",
    "                generated_char = self.idx_to_chr[generated_char_ind]\n",
    "                \n",
    "                #append this generated charachter to sample being generated. (we construct sample one by one character)\n",
    "                generated_sample.append(generated_char)\n",
    "        \n",
    "                #here we generate hot vector for already generated character and will use it as next input to network\n",
    "                x.zero_()\n",
    "                x[0,0,generated_char_ind] = 1.\n",
    "                \n",
    "                #if network generates end of line character, we stop generation for this sample and begin for next sample.\n",
    "                if generated_char=='\\n' or generated_char=='.':\n",
    "                    break\n",
    "            #generate sample string and save that sample to samples list\n",
    "            generated_samples_list.append(''.join(generated_sample))\n",
    "        \n",
    "            #prepare for next sample construction\n",
    "            generated_sample.clear()\n",
    "        \n",
    "        return generated_samples_list\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 1831\n"
     ]
    }
   ],
   "source": [
    "total_training_samples_num =  None\n",
    "batch_size = 64\n",
    "\n",
    "with open('The_old_man_and_sea.txt') as f:\n",
    "    whole_text = f.read()\n",
    "    alpha = set(whole_text.lower())\n",
    "    \n",
    "    idx_to_chr = {i:c for i,c in enumerate(alpha)}\n",
    "    chr_to_idx = {c:i for i,c in enumerate(alpha)}\n",
    "\n",
    "    samples = whole_text.split('.')\n",
    "    samples = [sentence.strip()+'.' for sentence in samples]\n",
    "    samples = [l.lower() for i,l in enumerate(samples) \n",
    "               if total_training_samples_num is None or i < total_training_samples_num]\n",
    "    del whole_text\n",
    "    print('Number of samples: {}'.format(len(samples)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "total_training_samples_num =  None\n",
    "batch_size = 16\n",
    "\n",
    "with open('shakespeare.txt') as f:\n",
    "    whole_text = f.read()\n",
    "    alpha = set(whole_text.lower())\n",
    "    del whole_text\n",
    "    idx_to_chr = {i:c for i,c in enumerate(alpha)}\n",
    "    chr_to_idx = {c:i for i,c in enumerate(alpha)}\n",
    "    f.seek(0)\n",
    "    samples = f.readlines()\n",
    "    samples = [l.lower() for i,l in enumerate(samples) \n",
    "               if total_training_samples_num is None or i < total_training_samples_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_size = len(alpha)\n",
    "hidden_size = 32\n",
    "output_size = dictionary_size\n",
    "num_layers = 2\n",
    "directions_num = 1\n",
    "is_bidirectional = directions_num == 2\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "number_of_samples_to_generate = 20\n",
    "max_sample_length = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_tensor_list = encode_samples_to_hot_tensors(samples,chr_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "module = LSTM_module(hidden_size,num_layers,is_bidirectional,idx_to_chr).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[':;-sln\\n',\n",
       " \";')-cjdodxmpabp'z!knefp:brtzeq.\",\n",
       " \"dzqcq'yq;ybjai?fx;?;nyehfy\\n\",\n",
       " \": ck:ij:('\\n\",\n",
       " \"ei!laedo!:azlw'qs,;gk\\n\",\n",
       " \"ppattlrax'pubc)kvtm\\n\",\n",
       " \"'.\",\n",
       " \"-!folqku: :;:( xvbkfw))'anj,dwy\\n\",\n",
       " 'o \\n',\n",
       " \"afu!yoai:r,p!j'ck'l- c?u;zfyjeibpuoh!i)foi?t;su\\n\",\n",
       " 'wj j.',\n",
       " 'o.',\n",
       " ',bx\\n',\n",
       " '-g:xww;,.',\n",
       " '.',\n",
       " '?qm;dvt!oieeqhd)-o.',\n",
       " \"jdnd'mwnil)!vbhawuclk?sst-xd:t,?-raa), ihricjrzug!r,ewodj\\n\",\n",
       " '!\\n',\n",
       " ',,g)ngcdg,lrca:e\\n',\n",
       " 'a?:kz xg\\n']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module.generate_samples(number_of_samples_to_generate,max_sample_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#module.load_state_dict(torch.load('shakspear_weights.pt'))\n",
    "#module.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "criteria = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.Adam(module.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 | loss 2.9665\n",
      "epoch 1 | loss 2.9690\n",
      "epoch 2 | loss 2.7861\n",
      "epoch 3 | loss 2.7381\n",
      "epoch 4 | loss 2.4473\n",
      "epoch 5 | loss 2.4562\n",
      "epoch 6 | loss 2.4798\n",
      "epoch 7 | loss 2.2690\n",
      "epoch 8 | loss 2.2710\n",
      "epoch 9 | loss 2.4637\n",
      "epoch 10 | loss 2.2025\n",
      "epoch 11 | loss 2.2511\n",
      "epoch 12 | loss 2.1704\n",
      "epoch 13 | loss 2.0900\n",
      "epoch 14 | loss 2.2844\n",
      "epoch 15 | loss 2.1794\n",
      "epoch 16 | loss 2.0733\n",
      "epoch 17 | loss 2.1078\n",
      "epoch 18 | loss 2.0334\n",
      "epoch 19 | loss 2.0210\n",
      "Training is finished.\n"
     ]
    }
   ],
   "source": [
    "module.train()\n",
    "for epoch in range(20):\n",
    "    #for each epoch here we create new batch generator object\n",
    "    batch_gen = batch_generator(samples_tensor_list,batch_size)\n",
    "    \n",
    "    #we take each batch from batch generator object\n",
    "    for batch in batch_gen:\n",
    "        x, y = batch_preprocess(batch)\n",
    "\n",
    "        y_hat, (h_n, c_n) = module(x.to(device))\n",
    "\n",
    "        loss = criteria(y_hat, y.data.to(device))\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "    print('epoch {} | loss {:.4f}'.format(epoch,loss.item()))\n",
    "\n",
    "print('Training is finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM_module(\n",
       "  (lstm): LSTM(38, 32, num_layers=2)\n",
       "  (linear): Linear(in_features=32, out_features=38, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"nease a(bisrangbun'nt tot fyeich mey fdild,he,\\n\",\n",
       " 'poun ant,wh deandt laviven, mor alt anesser,\\n',\n",
       " 'thoin unkenst of dude my toe,\\n',\n",
       " 'funr a lope on so she sorefthmont lov,\\n',\n",
       " '\\n',\n",
       " 'aros thn okthud, faldt yey oie,\\n',\n",
       " 'thos meint beent if nyiy touute sorted,\\n',\n",
       " 'at heet sjees xart hat solt thatt home mavisp,\\n',\n",
       " 'pun othy sherst mang as atose thze, temike theot.',\n",
       " 'frien cot treuce sulelt etsacfrott mejit,\\n',\n",
       " 'nririatereyt goeels,h le eltiygeves\\n',\n",
       " \"anll's wirr yimghin suln thley on-nfetne,\\n\",\n",
       " 'detirt hiru dy beide foy i nos the,\\n',\n",
       " 'to ne ellow tpan smiwed cat vy heor,\\n',\n",
       " 'to or that ceure aclrshe oro thourtelom,\\n',\n",
       " \"tian mininl's thou all sacipe leame sigudte.\",\n",
       " '\\n',\n",
       " 'thoal diwgurs in ame but stisist epipt wear,\\n',\n",
       " 'e\\n',\n",
       " 'nald fige thee i thag thup me thar cfenwelt,\\n']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module.generate_samples(number_of_samples_to_generate,max_sample_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(module.state_dict(),'shakspear_weights.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
